{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e703a43",
   "metadata": {},
   "source": [
    "## Evaluation of AlphaGenome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a9aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import math\n",
    "import pickle as pkl\n",
    "from itertools import combinations\n",
    "import csv\n",
    "import logging\n",
    "logger = logging.getLogger('main')\n",
    "logger.setLevel(logging.INFO)\n",
    "file_handler = logging.FileHandler('evaluation_alphagenome.log')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', \"%m-%d %H:%M\")\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "aligners=[\"star\"]\n",
    "samples=[\"J26675-L1_S1\"]\n",
    "chromosomes=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"X\",\"Y\"]\n",
    "\n",
    "# round std up and keep 2 significant digits\n",
    "# round mean to same precision as std. dev., if more than 3 digits after comma use e notation\n",
    "def log_round_mean_std(mean, std, tool, aligner, gt_confidence, score):\n",
    "\tnr_nks_std = int(f\"{std:.1e}\".split('e')[1][1:]) +  1\n",
    "\tstd += round(0.1**(nr_nks_std))\n",
    "\tstd = round(std, nr_nks_std)\n",
    "\tmean = round(mean, nr_nks_std)\n",
    "\tif std <= 0.009:\n",
    "\t\tlogger.debug(f\"{tool} {aligner} {gt_confidence} {score}: {mean} ± {std:.1E}\")\n",
    "\telse:\n",
    "\t\tlogger.debug(f\"{tool} {aligner} {gt_confidence} {score}: {mean} ± {std}\")\n",
    "\n",
    "\n",
    "# save mean No Skill AUPRC +- 1 std. dev. (over all tools for scenario) in file\n",
    "def calc_no_skill_auprc(tool_sjs, score_dir, aligner, gt_confidence):\n",
    "\tperc_positives = []\n",
    "\tfor tool, (_, sjs) in enumerate(tool_sjs.items()):\n",
    "\t\tfor _, sj in sjs.items():\n",
    "\t\t\tperc_positives.append(len(sj[sj['label']==1])/len(sj))\n",
    "\t# Since precision is constant for a No-Skill Classifier, AUPRC is the constant precision multiplied by the range of recall (which is 1)\n",
    "\tno_skill_auprc = np.mean(perc_positives)\n",
    "\tno_skill_std = np.std(perc_positives)\n",
    "\tlog_round_mean_std(no_skill_auprc, no_skill_std, 'No Skill', aligner, gt_confidence, 'AUPRC')\n",
    "\tout_file = f'{score_dir}/No_Skill_auprcs.csv'\n",
    "\tif (not os.path.isfile(out_file)):\n",
    "\t\twith open(out_file, 'w') as file:\n",
    "\t\t\tfile.write(f'aligner,gt_confidence,tool,mean_auprc,std_auprc\\n')\n",
    "\twith open(out_file, 'a') as f:\n",
    "\t\tf.write(f\"{aligner},{gt_confidence},No Skill,{no_skill_auprc},{no_skill_std}\\n\")\n",
    "\n",
    "\n",
    "def calc_f1_score_at_threshold(sjs, score_dir, aligner, gt_confidence, threshold=0.5):\n",
    "\tthreshold = round(threshold, 1) # round threshold to 1 decimal place\n",
    "\tthreshold_ = threshold \n",
    "\tf1_scores = {}\n",
    "\tfor ann, sj in sjs.items():\n",
    "\t\ttool, _,_ = ann\n",
    "\t\t# f1 score at given threshold\n",
    "\t\tif tool == 'deepsplice':\n",
    "\t\t\tthreshold_ = threshold * 1.7052 # scale DeepSplice scores to [0,1] range\n",
    "\t\tf1_scores[ann] = f1_score(sj['label'], sj['pred'] >= threshold_)\n",
    "\tmean_f1_score = np.mean(list(f1_scores.values()))\n",
    "\tstd_f1_score = np.std(list(f1_scores.values()))\n",
    "\t# if out_mean_file does not exist, create it and write header\n",
    "\tout_mean_file = f'{score_dir}/mean_std_f1_scores.csv'\n",
    "\tif (not os.path.isfile(out_mean_file)):\n",
    "\t\twith open(out_mean_file, 'w') as file:\n",
    "\t\t\tfile.write(f'aligner,gt_confidence,tool,mean_f1_score,std_f1_score,threshold\\n')\n",
    "\twith open(out_mean_file, 'a') as file:\n",
    "\t\tfile.write(f'{aligner},{gt_confidence},{tool},{mean_f1_score},{std_f1_score},{threshold}\\n')\n",
    "\tlog_round_mean_std(mean_f1_score, std_f1_score, tool, aligner, gt_confidence, f'F1 Score at {threshold}')\n",
    "\n",
    "\n",
    "# save mean AUPRC (area under the precision recall curve) scores +- 1 std. dev. and number positives and number negatives +- 1 std. dev. in file \n",
    "def calc_auprc(sjs, score_dir, aligner, gt_confidence):\n",
    "\tscores = {}\n",
    "\tnr_positives = []\n",
    "\tnr_negatives = []\n",
    "\tfor ann, sj in sjs.items():\n",
    "\t\ttool, _,_ = ann\n",
    "\t\tscore = average_precision_score(sj['label'], sj['pred'])\n",
    "\t\tscores[ann] = score\n",
    "\t\tnr_positives.append(len(sj[sj[\"label\"]==1]))\n",
    "\t\tnr_negatives.append(len(sj[sj[\"label\"]==0]))\n",
    "\tmean_auprc = np.mean(list(scores.values()))\n",
    "\tauprc_std = np.std(list(scores.values()))\n",
    "\t# if out_file does not exist, create it and write header\n",
    "\tout_file = f'{score_dir}/{tool}_{aligner}_{gt_confidence}_auprcs.csv'\n",
    "\twith open(out_file, 'w') as file:\n",
    "\t\tfile.write(f'tool,sample_id,run_nr,auprc\\n')\n",
    "\twith open(out_file, 'a') as file:\n",
    "\t\tfor ann, score in scores.items():\n",
    "\t\t\tfile.write(','.join(ann)+','+str(score)+'\\n')\n",
    "\t# if out_mean_file does not exist, create it and write header\n",
    "\tout_mean_file = f'{score_dir}/mean_std_auprcs.csv'\n",
    "\tif (not os.path.isfile(out_mean_file)):\n",
    "\t\twith open(out_mean_file, 'w') as file:\n",
    "\t\t\tfile.write(f'aligner,gt_confidence,tool,mean_auprc,std_auprc\\n')\n",
    "\twith open(out_mean_file, 'a') as file:\n",
    "\t\tfile.write(f'{aligner},{gt_confidence},{tool},{mean_auprc},{auprc_std}\\n')\n",
    "\tlog_round_mean_std(mean_auprc, auprc_std, tool, aligner, gt_confidence, 'AUPRC')\n",
    "\t# if out_nr_file does not exist, create it and write header\n",
    "\tout_nr_file = f'{score_dir}/mean_nr_positives_negatives.csv'\n",
    "\tif (not os.path.isfile(out_nr_file)):\n",
    "\t\twith open(out_nr_file, 'w') as file:\n",
    "\t\t\tfile.write(f'aligner,gt_confidence,tool,mean_nr_positives,std_nr_positives,mean_nr_negatives,std_nr_negatives\\n')\n",
    "\twith open(out_nr_file, 'a') as file:\n",
    "\t\tfile.write(f'{aligner},{gt_confidence},{tool},{math.floor(np.mean(nr_positives))},{math.floor(np.std(nr_positives))},{math.floor(np.mean(nr_negatives))},{math.floor(np.std(nr_negatives))}\\n')\n",
    "\n",
    "\n",
    "# save effectsize in file \n",
    "def write_effectsize_ci(tool_sjs, stats_dir, aligner, gt_confidence, n_bootstrap=1000):\n",
    "    TOOL_PLOT_NAME = {\n",
    "\t\t'alphagenome' :'AlphaGenome',\n",
    "        'spliceai': 'SpliceAI',\n",
    "        'deepsplice': 'DeepSplice',\n",
    "        'jcc': 'JCC',\n",
    "        'baseline': 'No-Skill'\n",
    "    }\n",
    "    # Tools to use (excluding baseline)\n",
    "    tool_names = [key for key in TOOL_PLOT_NAME if key in tool_sjs]\n",
    "\n",
    "    # Precompute bootstrap indices for every sample/run\n",
    "    bootstrap_indices = {}\n",
    "    lens = {}\n",
    "    for (_, sample, run_id), s in tool_sjs[tool_names[1]].items():\n",
    "        n = s.shape[0]\n",
    "        bootstrap_indices[(sample, run_id)] = np.random.randint(0, n, size=(n_bootstrap, n))\n",
    "        lens[(sample, run_id)] = n\n",
    "\n",
    "    # Precompute per-tool per-(sample,run) AUPRCs for each bootstrap\n",
    "    per_tool_auprcs = {t: {} for t in tool_names}\n",
    "    for t in tool_names:\n",
    "        for (_, sample, run_id), s in tool_sjs[t].items():\n",
    "            indices = bootstrap_indices[(sample, run_id)]\n",
    "            labels = s['label'].values\n",
    "            preds = s['pred'].values\n",
    "            boot_auprcs = np.array([\n",
    "                average_precision_score(labels[idx], preds[idx]) for idx in indices\n",
    "            ])\n",
    "            per_tool_auprcs[t][(sample, run_id)] = boot_auprcs\n",
    "\n",
    "    # Precompute baseline auprcs:\n",
    "    per_baseline_auprcs = {}\n",
    "    for (_, sample, run_id), s in tool_sjs[tool_names[1]].items():\n",
    "        indices = bootstrap_indices[(sample, run_id)]\n",
    "        labels = s['label'].values\n",
    "        n = len(labels)\n",
    "        # Baseline: predict all 1's\n",
    "        preds_baseline = np.ones_like(labels)\n",
    "        boot_auprcs = np.array([\n",
    "            average_precision_score(labels[idx], preds_baseline[idx]) for idx in indices\n",
    "        ])\n",
    "        per_baseline_auprcs[(sample, run_id)] = boot_auprcs\n",
    "\n",
    "    results_rows = []\n",
    "\n",
    "    # Compare each tool to baseline\n",
    "    for t in tool_names:\n",
    "        t_name = TOOL_PLOT_NAME[t]\n",
    "        for (sample, run_id) in per_tool_auprcs[t]:\n",
    "            tool_auprcs = per_tool_auprcs[t][(sample, run_id)]\n",
    "            baseline_auprcs = per_baseline_auprcs[(sample, run_id)]\n",
    "            bootstrapped_deltas = tool_auprcs - baseline_auprcs\n",
    "            mean_delta = np.mean(bootstrapped_deltas)\n",
    "            ci_lower, ci_upper = np.percentile(bootstrapped_deltas, [2.5, 97.5])\n",
    "            row = [aligner, gt_confidence, sample, run_id, t_name, TOOL_PLOT_NAME['baseline'], mean_delta, ci_lower, ci_upper]\n",
    "            results_rows.append(row)\n",
    "\n",
    "    # Compare all pairs of tools (excluding baseline)\n",
    "    for t1, t2 in combinations(tool_names, 2):\n",
    "        n1 = TOOL_PLOT_NAME[t1]\n",
    "        n2 = TOOL_PLOT_NAME[t2]\n",
    "        for (sample, run_id) in per_tool_auprcs[t1]:\n",
    "            auprc1 = per_tool_auprcs[t1][(sample, run_id)]\n",
    "            auprc2 = per_tool_auprcs[t2][(sample, run_id)]\n",
    "            bootstrapped_deltas = auprc1 - auprc2\n",
    "            mean_delta = np.mean(bootstrapped_deltas)\n",
    "            ci_lower, ci_upper = np.percentile(bootstrapped_deltas, [2.5, 97.5])\n",
    "            row = [aligner, gt_confidence, sample, run_id, n1, n2, mean_delta, ci_lower, ci_upper]\n",
    "            results_rows.append(row)\n",
    "\n",
    "    # Write to CSV\n",
    "    csv_file_path = f'{stats_dir}/effect_sizes_new_bootstrap.csv'\n",
    "    write_header = not os.path.exists(csv_file_path) or os.stat(csv_file_path).st_size == 0\n",
    "    with open(csv_file_path, mode='a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        if write_header:\n",
    "            headers = ['aligner', 'gt_confidence', 'sample', 'run_id', 'tool1', 'tool2', 'mean_delta', 'ci_lower', 'ci_upper']\n",
    "            writer.writerow(headers)\n",
    "        writer.writerows(results_rows)\n",
    "\n",
    "\n",
    "def write_operating_points(y_true, y_scores, stats_dir, tool, aligner, gt_confidence):\n",
    "\t# At fixed thresholds\n",
    "\tthresholds = np.array([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "\tsummary_file = f\"{stats_dir}/operating_points.csv\"\n",
    "\tif (not os.path.isfile(summary_file)):\n",
    "\t\twith open(summary_file, 'w') as file:\n",
    "\t\t\tfile.write(f'tool,aligner,gt_confidence,threshold,precision,recall,TP,FP,FN,TN\\n')\n",
    "\twith open(summary_file, 'a') as f:\n",
    "\t\tfor threshold in thresholds:\n",
    "\t\t\tpred_at_thresh = (y_scores >= threshold).astype(int)\n",
    "\t\t\tprecision_fixed = precision_score(y_true, pred_at_thresh, zero_division=0)\n",
    "\t\t\trecall_fixed = recall_score(y_true, pred_at_thresh, zero_division=0)\n",
    "\t\t\ttn, fp, fn, tp = confusion_matrix(y_true, pred_at_thresh).ravel()\n",
    "\t\t\tf.write(f\"{tool},{aligner},{gt_confidence},{threshold},{precision_fixed},{recall_fixed},{tp},{fp},{fn},{tn}\\n\")\n",
    "\n",
    "\n",
    "# calculate performance for Scenario 2: \"Predicting junctions that could be detected with higher sequencing depth\" / Scenario 3: \"Predicting hard-to-find junctions\"\n",
    "# for Real-world / Hypothetical setting\n",
    "def scenario_2_3(aligner, gt_confidence, stats_dir, scenario, hard_to_find, file_sj_50):\n",
    "\tos.makedirs(stats_dir, exist_ok=True)\n",
    "\tif os.path.isfile(file_sj_50):\n",
    "\t\twith open(file_sj_50, 'rb') as f:\n",
    "\t\t\ttool_sjs_50 = pkl.load(f)\n",
    "\t\tlogger.debug(f'Loaded {file_sj_50} from disk.')\n",
    "\telse:\n",
    "\t\tlogger.error(f'No file {file_sj_50}')\n",
    "\t\texit()\n",
    "\tfor tool, sjs in tool_sjs_50.items():\t\n",
    "\t\tall_labels = np.concatenate([sj[\"label\"] for sj in sjs.values()]) # append all samples\n",
    "\t\tall_preds = np.concatenate([sj[\"pred\"] for sj in sjs.values()]) # append all samples\n",
    "\t\twrite_operating_points(all_labels, all_preds, stats_dir, tool, aligner, gt_confidence)\n",
    "\t\tcalc_auprc(sjs, stats_dir, aligner, gt_confidence)\n",
    "\t\tfor threshold in np.arange(0, 1.1, 0.1):\n",
    "\t\t\tcalc_f1_score_at_threshold(sjs, stats_dir, aligner, gt_confidence, threshold=threshold)\n",
    "\tcalc_no_skill_auprc(tool_sjs_50, stats_dir, aligner, gt_confidence)\n",
    "\twrite_effectsize_ci(tool_sjs_50, stats_dir, aligner, gt_confidence)\n",
    "\n",
    "\n",
    "main_dir=\".\" #TODO adjust path\n",
    "out_dir = f'{main_dir}/out'\n",
    "data_dir = f'{main_dir}/data'\n",
    "pred_dir = f'{data_dir}/predictions'\n",
    "stats_dir = f'{out_dir}/stats_alphagenome'\n",
    "os.makedirs(stats_dir, exist_ok = True)\n",
    "\n",
    "for hard_to_find in [True]:\n",
    "\tfor scenario in ['hypothetical']:\n",
    "\t\tfor aligner, gt_confidence in [('star','unfiltered'),('star','illumina'),('star','cutoff')]:\n",
    "\t\t\tlogger.info(f'--------- RUN Scenario {3 if hard_to_find else 2} {scenario} vs GT {aligner} {gt_confidence} ---------')\n",
    "\t\t\tstats_dir_scenario = f'{stats_dir}/scenario_{3 if hard_to_find else 2}_{scenario}'\n",
    "\t\t\tos.makedirs(stats_dir_scenario, exist_ok = True)\n",
    "\t\t\tfile_sj_50 = f'{stats_dir_scenario}/tool_sjs_50_{aligner}_vs_GT_{aligner}_{gt_confidence}.pkl'\n",
    "\t\t\tscenario_2_3(aligner, gt_confidence, stats_dir, scenario, hard_to_find, file_sj_50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plotting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
